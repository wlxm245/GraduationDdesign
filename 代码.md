#data_clean.py

from pyspark.sql import SparkSession

if __name__ == '__main__':
    spark = SparkSession.builder. \
        appName("data_clean"). \
        master("yarn"). \
        config("spark.sql.shuffle.partitions", 2). \
        config("spark.sql.warehouse.dir", "hdfs://node1:8020/user/hive/warehouse"). \
        config("hive.metastore.uris", "thrift://node1:9083"). \
        enableHiveSupport(). \
        getOrCreate()
    sc = spark.sparkContext


    # 删除函数
    def clean(df):
        # 删除包含任何缺失值的行
        df_filled = df.na.drop()
        # 删除重复行
        df_unique = df_filled.dropDuplicates()
        return df_unique


    #对4个表处理
    df_courier_device = clean(spark.table("myhive.courier_device"))
    df_courier_device.write.mode("overwrite").saveAsTable("myhive.courier_device_clean")

    df_merchant_device = clean(spark.table("myhive.merchant_device"))
    df_merchant_device.write.mode("overwrite").saveAsTable("myhive.merchant_device_clean")

    df_test_courier_feedback = clean(spark.table("myhive.courier_feedback"))
    df_courier_feedback = df_test_courier_feedback.where("acceptance_hour >= 0 AND acceptance_minute >= 0 AND "
                                                         "acceptance_second >= 0 AND feedback_hour >= 0 AND "
                                                         "feedback_minute >= 0 AND feedback_second >= 0 AND feedback "
                                                         ">= 0 AND distance_to_merchant >= 0 AND "
                                                         "distance_to_merchant <= 1000")
    df_courier_feedback.write.mode("overwrite").saveAsTable("myhive.courier_feedback_clean")

    df_test_courier_report = clean(spark.table("myhive.courier_report"))
    df_courier_report = df_test_courier_report.where("acceptance_hour >= 0 AND acceptance_minute >= 0 AND "
                                                     "acceptance_second >= 0 AND arrier_hour >= 0 AND arrier_minute "
                                                     ">= 0 AND arrier_second >= 0 AND pickup_hour >= 0 AND "
                                                     "pickup_minute >= 0 AND pickup_second >= 0 AND delivery_hour >= "
                                                     "0 AND delivery_minute >= 0 AND delivery_second >= 0")
    df_courier_report.write.mode("overwrite").saveAsTable("myhive.courier_report_clean")


    #查看删除情况
    spark.table("myhive.courier_device").show()
    spark.table("myhive.courier_device_clean").show()
    spark.table("myhive.merchant_device").show()
    spark.table("myhive.merchant_device_clean").show()
    spark.table("myhive.courier_feedback").show()
    spark.table("myhive.courier_feedback_clean").show()
    spark.table("myhive.courier_report").show()
    spark.table("myhive.courier_report_clean").show()

    spark.stop()


#database_input_and_create.sql
create database if not exists myhive;

use myhive;

create table myhive.courier_device(
    no int comment '序号',
    city_id string comment '城市ID',
    day int comment '天数',
    courier_id string comment '快递员ID哈希值',
    os_version string comment '系统版本',
    phone_mode string comment '手机型号'
)row format delimited fields terminated by ','
tblproperties ('skip.header.line.count'='1');

create table myhive.merchant_device(
    no int comment '序号',
    city_id string comment '城市ID',
    day int comment '天数',
    shop_id string comment '商家ID哈希值',
    beacon_id string comment '信标ID哈希值',
    manufacturer string comment '制造商',
    brand string comment '品牌',
    model string comment '型号'
)row format delimited fields terminated by ','
tblproperties ('skip.header.line.count'='1');

create table myhive.courier_report(
    no int comment '序号',
    city_id string comment '城市ID',
    day int comment '天数',
    courier_id string comment '快递员ID哈希值',
    shop_id string comment '商家ID哈希值',
    acceptance_hour int comment '快递员接受订单时间（小时）',
    acceptance_minute int comment '快递员接受订单时间（分钟）',
    acceptance_second int comment '快递员接受订单时间（秒）',
    arrier_hour int comment '快递员到达商户时间（小时）',
    arrier_minute int comment '快递员到达商户时间（分钟）',
    arrier_second int comment '快递员到达商户时间（秒）',
    pickup_hour int comment '快递员取货时间（小时）',
    pickup_minute int comment '快递员取货时间（分钟）',
    pickup_second int comment '快递员取货时间（秒）',
    delivery_hour int comment '快递完成时间（小时）',
    delivery_minute int comment '快递完成时间（分钟）',
    delivery_second int comment '快递完成时间（秒）'
)row format delimited fields terminated by ','
tblproperties ('skip.header.line.count'='1');

create table myhive.courier_feedback(
    city_id string comment '城市ID',
    day int comment '天数',
    courier_id string comment '快递员ID哈希值',
    shop_id string comment '商家ID哈希值',
    acceptance_hour int comment '快递员接受订单时间（小时）',
    acceptance_minute int comment '快递员接受订单时间（分钟）',
    acceptance_second int comment '快递员接受订单时间（秒）',
    feedback_hour int comment '快递员响应通知时间（小时）',
    feedback_minute int comment '快递员响应通知时间（分钟）',
    feedback_second int comment '快递员响应通知时间（秒）',
    feedback int comment '快递员对通知的回应（0是稍后尝试，1是确认）',
    distance_to_merchant double comment '响应通知时快递员和商店的距离',
    gps_measure_offset int comment '反馈时间和gps时间的时间差'
)row format delimited fields terminated by ','
tblproperties ('skip.header.line.count'='1');


load data inpath '/tmp/file/Courier_device_data.csv' into table myhive.courier_device;

load data inpath '/tmp/file/Merchant_device_data.csv' into table myhive.merchant_device;

load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_1.csv' into table myhive.courier_feedback;
load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_2.csv' into table myhive.courier_feedback;
load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_3.csv' into table myhive.courier_feedback;
load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_4.csv' into table myhive.courier_feedback;
load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_5.csv' into table myhive.courier_feedback;
load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_6.csv' into table myhive.courier_feedback;
load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_7.csv' into table myhive.courier_feedback;
load data inpath '/tmp/file/Courier_feedback_data.csv/Courier_feedback_data_city_8.csv' into table myhive.courier_feedback;

load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_1.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_2.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_3.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_4.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_5.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_6.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_7.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_8.csv' into table myhive.courier_report;
load data inpath '/tmp/file/Courier_report_data.csv/Courier_report_data_city_9.csv' into table myhive.courier_report;


-- create table myhive.courier_device_clean(
--     no int comment '序号',
--     city_id string comment '城市ID',
--     day int comment '天数',
--     courier_id string comment '快递员ID哈希值',
--     os_version string comment '系统版本',
--     phone_mode string comment '手机型号'
-- )row format delimited fields terminated by ',';
--
-- create table myhive.merchant_device_clean(
--     no int comment '序号',
--     city_id string comment '城市ID',
--     day int comment '天数',
--     shop_id string comment '商家ID哈希值',
--     beacon_id string comment '信标ID哈希值',
--     manufacturer string comment '制造商',
--     brand string comment '品牌',
--     model string comment '型号'
-- )row format delimited fields terminated by ',';
--
-- create table myhive.courier_report_clean(
--     no int comment '序号',
--     city_id string comment '城市ID',
--     day int comment '天数',
--     courier_id string comment '快递员ID哈希值',
--     shop_id string comment '商家ID哈希值',
--     acceptance_hour int comment '快递员接受订单时间（小时）',
--     acceptance_minute int comment '快递员接受订单时间（分钟）',
--     acceptance_second int comment '快递员接受订单时间（秒）',
--     arrier_hour int comment '快递员到达商户时间（小时）',
--     arrier_minute int comment '快递员到达商户时间（分钟）',
--     arrier_second int comment '快递员到达商户时间（秒）',
--     pickup_hour int comment '快递员取货时间（小时）',
--     pickup_minute int comment '快递员取货时间（分钟）',
--     pickup_second int comment '快递员取货时间（秒）',
--     delivery_hour int comment '快递完成时间（小时）',
--     delivery_minute int comment '快递完成时间（分钟）',
--     delivery_second int comment '快递完成时间（秒）'
-- )row format delimited fields terminated by ',';
--
-- create table myhive.courier_feedback_clean(
--     city_id string comment '城市ID',
--     day int comment '天数',
--     courier_id string comment '快递员ID哈希值',
--     shop_id string comment '商家ID哈希值',
--     acceptance_hour int comment '快递员接受订单时间（小时）',
--     acceptance_minute int comment '快递员接受订单时间（分钟）',
--     acceptance_second int comment '快递员接受订单时间（秒）',
--     feedback_hour int comment '快递员响应通知时间（小时）',
--     feedback_minute int comment '快递员响应通知时间（分钟）',
--     feedback_second int comment '快递员响应通知时间（秒）',
--     feedback int comment '快递员对通知的回应（0是稍后尝试，1是确认）',
--     distance_to_merchant double comment '响应通知时快递员和商店的距离',
--     gps_measure_offset int comment '反馈时间和gps时间的时间差'
-- )row format delimited fields terminated by ',';

#courier_analysis.py
from pyspark.sql.types import Row, DoubleType, StructType, StructField
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, desc, unix_timestamp, lpad, concat_ws, when, expr

if __name__ == '__main__':
    spark = SparkSession.builder. \
        appName("courier_analysis"). \
        master("yarn"). \
        config("spark.sql.shuffle.partitions", 2). \
        config("spark.sql.warehouse.dir", "hdfs://node1:8020/user/hive/warehouse"). \
        config("hive.metastore.uris", "thrift://node1:9083"). \
        enableHiveSupport(). \
        getOrCreate()
    sc = spark.sparkContext

    # 将feedback改成中文
    feedback_mapping = {
        0: "稍后尝试",
        1: "确认"
    }
    feedback_mapping_df = spark.createDataFrame([Row(feedback=k, feedback_tran=v) for k, v in feedback_mapping.items()])

    # 读取表数据
    courier_feedback_df = spark.table("myhive.courier_feedback_clean")
    courier_report_df = spark.table("myhive.courier_report_clean")

    # 计算响应时间（以秒为单位）
    response_time_seconds = (col("feedback_hour") * 3600 + col("feedback_minute") * 60 + col("feedback_second")) - \
                            (col("acceptance_hour") * 3600 + col("acceptance_minute") * 60 + col("acceptance_second"))

    # 添加响应时间列到 DataFrame
    courier_feedback_with_response_time_df = courier_feedback_df.withColumn("response_time_seconds",
                                                                            response_time_seconds)
    courier_feedback_with_response_time_df = courier_feedback_with_response_time_df.join(feedback_mapping_df,
                                                                                         on=["feedback"],
                                                                                         how="inner")

    # 计算平均响应时间
    avg_response_time = courier_feedback_with_response_time_df.agg({"response_time_seconds": "avg"}).collect()[0][0]
    print("平均响应时间: {:.2f} 秒".format(avg_response_time))

    # 手动定义模式
    schema = StructType([
        StructField("avg_response_second", DoubleType()),
        StructField("avg_response_minute", DoubleType())
    ])

    # 将平均响应时间写入hive表
    spark.createDataFrame([(avg_response_time, avg_response_time / 60)], schema) \
        .write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable("myhive_detail.avg_response_time")

    # 分析快递员对通知的回应情况
    response_counts = courier_feedback_with_response_time_df.groupBy("courier_id", "feedback_tran").count() \
        .withColumnRenamed("count", "feedback_count")
    response_counts.where("feedback_tran = '稍后尝试'").orderBy(desc("feedback_count")).show()
    response_counts.where("feedback_tran = '确认'").orderBy(desc("feedback_count")).show()

    # 将分析快递员对通知的回应情况写入hive表
    response_counts.write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable(
        "myhive_detail.courier_response_counts")

    # 分析快递员处理的订单数量
    order_counts = courier_feedback_with_response_time_df.groupBy("courier_id").count() \
        .withColumnRenamed("count", "order_count")
    order_counts.orderBy(desc("order_count")).show()

    # 计算每个外卖员的工作时长（以秒为单位）
    courier_report_df = courier_report_df.withColumn("acceptance_time",
                                                     concat_ws(':', lpad(col("acceptance_hour"), 2, '0'),
                                                               lpad(col("acceptance_minute"), 2, '0'),
                                                               lpad(col("acceptance_second"), 2, '0')))
    courier_report_df = courier_report_df.withColumn("delivery_time",
                                                     concat_ws(':', lpad(col("delivery_hour"), 2, '0'),
                                                               lpad(col("delivery_minute"), 2, '0'),
                                                               lpad(col("delivery_second"), 2, '0')))
    # 将时间字符串转换为时间戳，并计算时间差（秒）
    work_duration_seconds = (
            unix_timestamp(col("delivery_time"), "HH:mm:ss") -
            unix_timestamp(col("acceptance_time"), "HH:mm:ss")).cast("bigint")
    # 考虑跨越天数的情况，处理负数时长
    work_duration_seconds = when(work_duration_seconds < 0, 24 * 3600 + work_duration_seconds).otherwise(
        work_duration_seconds)

    # 添加工作时长列到 DataFrame
    courier_report_with_work_duration_df = courier_report_df.withColumn("work_duration_seconds", work_duration_seconds)

    # 计算每个外卖员的平均工作时长（以秒为单位）
    avg_work_duration_seconds = courier_report_with_work_duration_df.groupBy("courier_id") \
        .agg({"work_duration_seconds": "avg"}) \
        .withColumnRenamed("avg(work_duration_seconds)", "avg_work_duration_seconds")
    avg_work_duration_seconds.show()

    # 将快递员数据整合起来
    courier_final = order_counts.join(avg_work_duration_seconds, on=["courier_id"], how="inner")
    courier_final.show()

    # 计算avg_work_duration_seconds和order_count列的99%和1%分位数
    percentiles = courier_final.approxQuantile(["avg_work_duration_seconds", "order_count"], [0.01, 0.99], 0.001)

    # 提取分位数
    p01_work_duration_seconds, p99_work_duration_seconds = percentiles[0]
    p10_order_count, p90_order_count = percentiles[1]

    # 使用when函数将每列的值映射到0到100的范围内
    courier_final = courier_final.withColumn("work_duration_score",
                                             expr("CASE WHEN avg_work_duration_seconds <= {} THEN 100 "
                                                  "WHEN avg_work_duration_seconds >= {} THEN 0 "
                                                  "ELSE 100 * ({} - avg_work_duration_seconds) / ({} - {}) END"
                                                  .format(p01_work_duration_seconds, p99_work_duration_seconds,
                                                          p99_work_duration_seconds, p99_work_duration_seconds,
                                                          p01_work_duration_seconds)))

    courier_final = courier_final.withColumn("order_count_score",
                                             expr("CASE WHEN order_count <= {} THEN 0 "
                                                  "WHEN order_count >= {} THEN 100 "
                                                  "ELSE 100 * (order_count - {}) / ({} - {}) END"
                                                  .format(p10_order_count, p90_order_count,
                                                          p10_order_count, p90_order_count,
                                                          p10_order_count)))

    # 计算得分的平均值
    courier_final = courier_final.withColumn("final_score",
                                             (col("work_duration_score") * 0.4) + (col("order_count_score") * 0.6))
    courier_final = courier_final.drop(*('work_duration_score', 'order_count_score'))
    courier_final.orderBy(desc("final_score")).show()

    # 将快递员数据整合写入hive表
    courier_final.write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable(
        "myhive_detail.courier_final")

    # 停止SparkSession
    spark.stop()

#delivery_time.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, concat_ws, unix_timestamp, lpad, avg, desc, rank, count
from pyspark.sql.types import Row
from pyspark.sql.window import Window

if __name__ == '__main__':
    spark = SparkSession.builder. \
        appName("delivery_time"). \
        master("yarn"). \
        config("spark.sql.shuffle.partitions", 2). \
        config("spark.sql.warehouse.dir", "hdfs://node1:8020/user/hive/warehouse"). \
        config("hive.metastore.uris", "thrift://node1:9083"). \
        enableHiveSupport(). \
        getOrCreate()
    sc = spark.sparkContext

    # 创建city_mapping字典
    city_mapping = {
        1: '上海',
        2: '杭州',
        3: '深圳',
        4: '北京',
        5: '潍坊',
        6: '广州',
        7: '遂宁',
        8: '西宁',
        9: '定安',
        10: '廊坊'
    }

    # 将字典转换为DataFrame
    city_mapping_df = spark.createDataFrame([Row(city_id=k, city_name=v) for k, v in city_mapping.items()])

    # 读取Hive表
    courier_report_df = spark.table("myhive.courier_report_clean")

    # 计算接受订单和完成配送的时间戳
    courier_report_df = courier_report_df.withColumn("acceptance_time",
                                                     concat_ws(':', lpad(col("acceptance_hour"), 2, '0'),
                                                               lpad(col("acceptance_minute"), 2, '0'),
                                                               lpad(col("acceptance_second"), 2, '0')))
    courier_report_df = courier_report_df.withColumn("delivery_time",
                                                     concat_ws(':', lpad(col("delivery_hour"), 2, '0'),
                                                               lpad(col("delivery_minute"), 2, '0'),
                                                               lpad(col("delivery_second"), 2, '0')))
    # 将时间字符串转换为时间戳，并计算时间差（秒）
    courier_report_df = courier_report_df.withColumn("total_delivery_time_seconds",
                                                     (unix_timestamp(col("delivery_time"), "HH:mm:ss") - unix_timestamp(
                                                         col("acceptance_time"), "HH:mm:ss")).cast("bigint"))

    # 显示结果
    courier_report_df.show()

    # 分析配送流程效率，例如计算平均配送时间
    average_delivery_time = courier_report_df.agg({"total_delivery_time_seconds": "avg"}).collect()[0][0]
    print(f"平均配送时间: {average_delivery_time} 秒（{average_delivery_time / 60} 分钟）")

    # 将平均配送时间写入Hive表
    spark.createDataFrame([(average_delivery_time, average_delivery_time / 60)],
                          ["average_delivery_second", "average_delivery_minute"]) \
        .write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable(
        "myhive_detail.average_delivery_time")

    # 聚合分析，按小时计算平均配送时间
    average_delivery_time_per_hour = courier_report_df.groupBy("city_id", "acceptance_hour") \
        .agg(avg("total_delivery_time_seconds").alias("avg_delivery_time_seconds"),
             count("total_delivery_time_seconds").alias("count")).where("count > 1000")

    # 定义窗口规范，按城市进行分区，并按订单数量降序排序
    window_spec = Window.partitionBy("city_id").orderBy(desc("avg_delivery_time_seconds"))

    # 添加排名列
    ranked_df = average_delivery_time_per_hour.withColumn("rank", rank().over(window_spec))

    # 过滤出每个城市排名在前三的时间段
    top_busy_hours_per_city_df = ranked_df.where(col("rank") <= 3).orderBy(desc("avg_delivery_time_seconds")) \
        .join(city_mapping_df, on=["city_id"], how="inner") \
        .select("city_name", "acceptance_hour", "avg_delivery_time_seconds", "count")
    top_busy_hours_per_city_df.show()

    # 存到hive表
    top_busy_hours_per_city_df.write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable(
        "myhive_detail.top_busy_hours_per_city")

    # 停止SparkSession
    spark.stop()

#phone_model.py
from pyspark.sql import SparkSession
from pyspark.sql.types import IntegerType, Row

if __name__ == '__main__':
    spark = SparkSession.builder. \
        appName("phone_model"). \
        master("yarn"). \
        config("spark.sql.shuffle.partitions", 2). \
        config("spark.sql.warehouse.dir", "hdfs://node1:8020/user/hive/warehouse"). \
        config("hive.metastore.uris", "thrift://node1:9083"). \
        enableHiveSupport(). \
        getOrCreate()
    sc = spark.sparkContext

    df_courier = spark.table("myhive.courier_device_clean")
    df_courier = df_courier.withColumn("city_id", df_courier["city_id"].cast(IntegerType()))
    df_courier.createOrReplaceTempView("courier_view")

    df_merchant = spark.table("myhive.merchant_device_clean")
    df_merchant = df_merchant.withColumn("city_id", df_merchant["city_id"].cast(IntegerType()))
    df_merchant.createOrReplaceTempView("shop_view")

    # 创建city_mapping字典
    city_mapping = {
        1: '上海',
        2: '杭州',
        3: '深圳',
        4: '北京',
        5: '潍坊',
        6: '广州',
        7: '遂宁',
        8: '西宁',
        9: '定安',
        10: '廊坊'
    }

    # 将字典转换为DataFrame
    city_mapping_df = spark.createDataFrame([Row(city_id=k, city_name=v) for k, v in city_mapping.items()])

    # 统计快递员操作系统版本和手机型号的使用频率
    print("courier_phone_mode_counts")
    courier_phone_mode_counts = spark.sql("""
        SELECT phone_mode, COUNT(*) as count
        FROM courier_view
        GROUP BY phone_mode
        ORDER BY count DESC
        LIMIT 20
    """)
    courier_phone_mode_counts.show()
    # 将数据存入hive中
    courier_phone_mode_counts.write.mode("overwrite").option("delimiter", ",").format("orc") \
        .saveAsTable("myhive_detail.courier_phone_mode_counts")

    print("courier_os_version_counts")
    courier_os_version_counts = spark.sql("""
        SELECT phone_mode, os_version, COUNT(*) as count
        FROM courier_view
        GROUP BY phone_mode, os_version
        ORDER BY count DESC
        LIMIT 20
    """)
    courier_os_version_counts.show()
    # 将数据存入hive中
    courier_os_version_counts.write.mode("overwrite").option("delimiter", ",").format("orc") \
        .saveAsTable("myhive_detail.courier_os_version_counts")

    print("courier_city_phone_mode_favourite")
    courier_city_phone_mode_favourite = spark.sql("""
        SELECT city_id, phone_mode, count
        FROM (
        SELECT city_id, phone_mode, COUNT(*) as count,
        ROW_NUMBER() OVER (PARTITION BY city_id ORDER BY COUNT(*) DESC) as rn
        FROM courier_view
        GROUP BY city_id, phone_mode
        ) t
        WHERE t.rn = 1
        order by city_id ASC
        LIMIT 20
    """).join(city_mapping_df, on=["city_id"], how="inner").select("city_name", "phone_mode", "count")
    courier_city_phone_mode_favourite.show()
    # 将数据存入hive中
    courier_city_phone_mode_favourite.write.mode("overwrite").option("delimiter", ",").format("orc") \
        .saveAsTable("myhive_detail.courier_city_phone_mode_favourite")

    # 统计商家制造商和品牌的使用频率
    print("shop_brand_counts")
    shop_brand_counts = spark.sql("""
        SELECT brand, COUNT(*) as count
        FROM shop_view
        GROUP BY brand
        ORDER BY count DESC
        LIMIT 20
    """)
    shop_brand_counts.show()
    # 将数据存入hive中
    shop_brand_counts.write.mode("overwrite").option("delimiter", ",").format("orc") \
        .saveAsTable("myhive_detail.shop_brand_counts")

    print("shop_model_counts")
    shop_model_counts = spark.sql("""
        SELECT model, COUNT(*) as count
        FROM shop_view
        GROUP BY model
        ORDER BY count DESC
        LIMIT 20
    """)
    shop_model_counts.show()
    # 将数据存入hive中
    shop_model_counts.write.mode("overwrite").option("delimiter", ",").format("orc") \
        .saveAsTable("myhive_detail.shop_model_counts")

    print("shop_city_model_favourite")
    shop_city_model_favourite = spark.sql("""
        SELECT city_id, model, count
        FROM (
        SELECT city_id, model, COUNT(*) as count,
        ROW_NUMBER() OVER (PARTITION BY city_id ORDER BY COUNT(*) DESC) as rn
        FROM shop_view
        GROUP BY city_id, model
        ) 
        WHERE t.rn = 1
        order by city_id ASC
        LIMIT 20
    """).join(city_mapping_df, on=["city_id"], how="inner").select("city_name", "model", "count")
    shop_city_model_favourite.show()
    # 将数据存入hive中
    shop_city_model_favourite.write.mode("overwrite").option("delimiter", ",").format("orc") \
        .saveAsTable("myhive_detail.shop_city_model_favourite")

    # 对每个城市的外卖员和商家数量统计
    courier_counts = spark.sql("SELECT city_id, COUNT(*) AS courier_count FROM courier_view GROUP BY city_id")
    merchant_counts = spark.sql("SELECT city_id, COUNT(*) AS merchant_count FROM shop_view GROUP BY city_id")
    city_counts = courier_counts.join(merchant_counts, "city_id", "inner") \
        .join(city_mapping_df, on=["city_id"], how="inner").select("city_name", "courier_count", "merchant_count")
    city_counts.show()
    city_counts.write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable(
        "myhive_detail.city_courier_merchant_counts")

    spark.stop()


#shop_analysis.py
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import desc, rank, col

if __name__ == '__main__':
    spark = SparkSession.builder. \
        appName("shop_analysis"). \
        master("yarn"). \
        config("spark.sql.shuffle.partitions", 2). \
        config("spark.sql.warehouse.dir", "hdfs://node1:8020/user/hive/warehouse"). \
        config("hive.metastore.uris", "thrift://node1:9083"). \
        enableHiveSupport(). \
        getOrCreate()
    sc = spark.sparkContext

    # 读取表数据
    courier_feedback_df = spark.table("myhive.courier_feedback_clean")
    courier_report_df = spark.table("myhive.courier_report_clean")

    # 分析商家处理的订单数量
    shop_order_counts = courier_feedback_df.groupBy("shop_id").count() \
        .withColumnRenamed("count", "order_count")
    shop_order_counts.orderBy(desc("order_count")).show()

    # 写入到hive表
    shop_order_counts.write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable(
        "myhive_detail.shop_order_counts")

    # 分析每天最高销量的三个商家
    shop_top_by_day_df = courier_feedback_df.groupBy(["day", "shop_id"]).count() \
        .withColumnRenamed("count", "shop_day_count")

    # 定义窗口规范，按城市进行分区，并按订单数量降序排序
    window_spec = Window.partitionBy("day").orderBy(desc("shop_day_count"))

    # 添加排名列
    ranked_df = shop_top_by_day_df.withColumn("rank", rank().over(window_spec))

    # 过滤出每个城市排名在前三的时间段
    shop_top_by_day = ranked_df.where(col("rank") <= 3).orderBy(desc("shop_day_count"))
    shop_top_by_day = shop_top_by_day.orderBy(col("day").asc(), col("rank").asc()).select("day", "shop_id",
                                                                                          "shop_day_count")
    shop_top_by_day.show()

    # 写入到hive表
    shop_top_by_day.write.mode("overwrite").option("delimiter", ",").format("orc").saveAsTable(
        "myhive_detail.shop_top_by_day")

    # 结束spark
    spark.stop()

#courier_delivery_time_prediction.py
import time
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, OneHotEncoder, MinMaxScaler
from pyspark.ml.regression import GBTRegressor, RandomForestRegressor, LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.sql.functions import when, lit, col
from pyspark.sql import DataFrame


# 评价模型
def evaluate_predictions(predictions, name):
    # 计算MAE、MSE、RMSE和R-Squared等
    evaluator = RegressionEvaluator(labelCol="delivery_time", predictionCol=name)
    mae = evaluator.evaluate(predictions, {evaluator.metricName: "mae"})
    mse = evaluator.evaluate(predictions, {evaluator.metricName: "mse"})
    rmse = evaluator.evaluate(predictions, {evaluator.metricName: "rmse"})
    r2 = evaluator.evaluate(predictions, {evaluator.metricName: "r2"})

    # 输出指标结果
    print(f"MAE:{mae}       MSE:{mse}       RMSE:{rmse}         R-Squared:{r2}")

    return mae, mse, rmse, r2


# 将时间转换成时间戳
def convert_to_timestamp(df: DataFrame, time_cols: list) -> DataFrame:
    for col_name in time_cols:
        df = df.withColumn(f"{col_name}_time", col(f"{col_name}_hour") * 60 + col(f"{col_name}_minute"))
        df = df.withColumn(f"{col_name}_time", col(f"{col_name}_time").cast("int"))
        if col_name != "acceptance":
            df = df.withColumn(f"{col_name}_time",
                               when((col("acceptance_time") > 1080) & (col(f"{col_name}_time") < 120),
                                    col(f"{col_name}_time") + 1440).otherwise(
                                   col(f"{col_name}_time"))
                               )
    return df


if __name__ == '__main__':
    spark = SparkSession.builder. \
        appName("courier_delivery_time_prediction"). \
        master("yarn"). \
        config("spark.sql.shuffle.partitions", 2). \
        config("spark.sql.warehouse.dir", "hdfs://node1:8020/user/hive/warehouse"). \
        config("hive.metastore.uris", "thrift://node1:9083"). \
        enableHiveSupport(). \
        getOrCreate()
    sc = spark.sparkContext

    # 读取快递报告数据
    courier_report_df = spark.table("myhive.courier_report_clean")
    courier_feedback_df = spark.table("myhive.courier_feedback_clean")

    # 删除非必要数据
    courier_report_df = courier_report_df.drop("no")

    # 使用预测的数据
    data_df = courier_report_df.join(courier_feedback_df,
                                     on=["city_id", "day", "courier_id", "shop_id", "acceptance_hour",
                                         "acceptance_minute",
                                         "acceptance_second"],
                                     how="left")
    data_df = data_df.withColumn("has_system_notification",
                                 when((data_df["feedback_hour"].isNull())
                                      , lit(0)).otherwise(lit(1)))
    data_df = data_df.fillna({"feedback": 2, "distance_to_merchant": 0, "gps_measure_offset": 0, "feedback_hour": 24,
                              "feedback_minute": 0, "feedback_second": 0})
    data_df = data_df.withColumn("city_id", col("city_id").cast("int"))

    # 调用函数进行转换
    time_columns = ["acceptance", "arrier", "pickup", "feedback", "delivery"]
    data_df = convert_to_timestamp(data_df, time_columns)

    data_df = data_df.drop(*(
        "acceptance_hour", "acceptance_minute", "acceptance_second", "arrier_hour", "arrier_minute", "arrier_second",
        "pickup_hour", "pickup_minute", "pickup_second", "feedback_hour", "feedback_minute", "feedback_second",
        "delivery_hour", "delivery_minute", "delivery_second"))

    # 进一步处理特征向量
    # 创建OneHotEncoder将索引编码成独热向量
    encoder = OneHotEncoder(inputCol="city_id", outputCol="city_id_encoded")
    data_df = encoder.fit(data_df).transform(data_df)

    data_df.show()

    # 特征选择
    feature_columns = ['city_id_encoded', "day", 'acceptance_time', 'arrier_time', 'pickup_time', "feedback_time",
                       "feedback",
                       "distance_to_merchant", "gps_measure_offset", "has_system_notification"]

    # 合并特征列为一个特征向量
    assembler = VectorAssembler(inputCols=feature_columns, outputCol="raw_features")
    data = assembler.transform(data_df)
    data.show()

    # 归一化特征向量
    scaler = MinMaxScaler(inputCol="raw_features", outputCol="features")
    scaler_model = scaler.fit(data)
    data = scaler_model.transform(data)

    # 划分数据集为训练集和测试集
    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)

    # 初始化线性回归模型
    lr_model = LinearRegression(featuresCol="features", labelCol="delivery_time", predictionCol="prediction_lr")

    # 获取训练开始时间
    start_time = time.time()

    # 训练模型
    lr_trained_model = lr_model.fit(train_data)

    # 获取训练结束时间
    end_time = time.time()

    # 计算训练时间
    training_time = end_time - start_time
    print("线性回归训练时间:", training_time, "seconds")

    # 在测试集上进行预测
    lr_predictions = lr_trained_model.transform(test_data)
    lr_predictions_train = lr_trained_model.transform(train_data)

    # 评估模型性能
    print("线性回归 test_data 评估")
    evaluate_predictions(lr_predictions, "prediction_lr")
    print("线性回归 train_data 评估")
    evaluate_predictions(lr_predictions_train, "prediction_lr")

    # 初始化随机森林回归模型
    rf_model = RandomForestRegressor(featuresCol="features", labelCol="delivery_time",
                                     predictionCol="prediction_rf")

    # 获取训练开始时间
    start_time = time.time()

    # 训练模型
    trained_model = rf_model.fit(train_data)

    # 获取训练结束时间
    end_time = time.time()

    # 计算训练时间
    training_time = end_time - start_time
    print("随机森林训练时间:", training_time, "seconds")

    # 在测试集上进行预测
    rf_predictions = trained_model.transform(test_data)
    rf_predictions_train = trained_model.transform(train_data)

    # 评估模型性能
    print("随机森林test_data评估")
    evaluate_predictions(rf_predictions, "prediction_rf")
    print("随机森林train_data评估")
    evaluate_predictions(rf_predictions_train, "prediction_rf")

    # 初始化 GBTRegressor 模型
    gbt = GBTRegressor(featuresCol="features", labelCol="delivery_time", predictionCol="prediction_gbt")

    # 获取训练开始时间
    start_time = time.time()

    # 训练模型
    gbt_model = gbt.fit(train_data)

    # 获取训练结束时间
    end_time = time.time()

    # 计算训练时间
    training_time = end_time - start_time
    print("GBT训练时间:", training_time, "seconds")

    # 在测试集上进行预测
    gbt_predictions = gbt_model.transform(test_data)
    gbt_predictions_train = gbt_model.transform(train_data)

    # 评估模型
    # 评估模型性能
    print("GBT test_data评估")
    evaluate_predictions(gbt_predictions, "prediction_gbt")
    print("GBT train_data评估")
    evaluate_predictions(gbt_predictions_train, "prediction_gbt")

    # 使用均值融合模型
    fusion_predictions = rf_predictions.join(gbt_predictions, on=["city_id", "day", "courier_id", "shop_id", "feedback",
                                                                  "distance_to_merchant", "gps_measure_offset",
                                                                  "has_system_notification", "acceptance_time",
                                                                  "arrier_time", "pickup_time", "feedback_time",
                                                                  "delivery_time", "city_id_encoded", "features"],
                                             how="inner")
    fusion_predictions = fusion_predictions.join(lr_predictions,
                                                 on=["city_id", "day", "courier_id", "shop_id", "feedback",
                                                     "distance_to_merchant", "gps_measure_offset",
                                                     "has_system_notification", "acceptance_time",
                                                     "arrier_time", "pickup_time", "feedback_time",
                                                     "delivery_time", "city_id_encoded", "features"],
                                                 how="inner") \
        .withColumn("fusion_prediction", (col("prediction_rf") + col("prediction_gbt") + col("prediction_lr")) / 3.0)

    print("融合模型test_data评估")
    evaluate_predictions(fusion_predictions, "fusion_prediction")

    fusion_predictions_train = rf_predictions_train.join(gbt_predictions_train,
                                                         on=["city_id", "day", "courier_id", "shop_id", "feedback",
                                                             "distance_to_merchant", "gps_measure_offset",
                                                             "has_system_notification", "acceptance_time",
                                                             "arrier_time", "pickup_time", "feedback_time",
                                                             "delivery_time", "city_id_encoded", "features"],
                                                         how="inner")
    fusion_predictions_train = fusion_predictions_train.join(lr_predictions_train,
                                                             on=["city_id", "day", "courier_id", "shop_id", "feedback",
                                                                 "distance_to_merchant", "gps_measure_offset",
                                                                 "has_system_notification", "acceptance_time",
                                                                 "arrier_time", "pickup_time", "feedback_time",
                                                                 "delivery_time", "city_id_encoded", "features"],
                                                             how="inner") \
        .withColumn("fusion_prediction", (col("prediction_rf") + col("prediction_gbt") + col("prediction_lr")) / 3.0)
    print("融合模型train_data评估")
    evaluate_predictions(fusion_predictions_train, "fusion_prediction")

    # 结束 SparkSession
    spark.stop()

